
<!DOCTYPE html>

<html lang="zh_CN">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17: http://docutils.sourceforge.net/" />

    <title>第五章 机器学习 &#8212; XIA&#39;s BLOG 1.0 文档</title>
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/translations.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="索引" href="genindex.html" />
    <link rel="search" title="搜索" href="search.html" />
    <link rel="next" title="第六章 文献阅读笔记" href="%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB_C6.html" />
    <link rel="prev" title="第四章 电脑操作" href="%E7%94%B5%E8%84%91%E6%93%8D%E4%BD%9C%E7%9B%B8%E5%85%B3_C4.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="id1">
<h1>第五章 机器学习<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h1>
<section id="id2">
<h2>5.1 稀疏自编码器<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h2>
<p><strong>References:</strong></p>
<p><a class="reference external" href="http://web.stanford.edu/class/archive/cs/cs294a/cs294a.1104/sparseAutoencoder.pdf">[1]CS294A Lecture notes</a></p>
<p><a class="reference external" href="https://www.zhihu.com/question/41490383/answer/103006793">[2]一文看懂自编码器、堆叠自编码器、稀疏自编码器、降噪自编码器</a></p>
<p><a class="reference external" href="https://en.m.wikipedia.org/wiki/Autoencoder#Regularized_autoencoders">[3]Regularized_autoencoders —— Sparse Autoencoder</a></p>
<hr class="docutils" />
<p><strong>ContentJumpTo:</strong></p>
<ul class="simple">
<li><p><a class="reference internal" href="#id4">降噪自编码器原理图</a></p></li>
<li><p><a class="reference internal" href="#no1">NO1</a></p></li>
<li><p><a class="reference internal" href="#no2">NO2</a></p></li>
<li><p><a class="reference internal" href="#no3">NO3</a></p></li>
</ul>
<hr class="docutils" />
<p><strong>Contents:</strong></p>
<p id="no1"><strong>NO1: 一文看懂自编码器、堆叠自编码器、稀疏自编码器、降噪自编码器</strong>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>1. 自编码器
   - 如果编码和解码的函数是恒等映射，那么自编码器相当于没有学习到任何新的东西，自始至终只有输入信息；
   - 基于上一点，为了防止恒等映射的发生，我们经常对中间信号（编码）做一定的约束，这样系统能学到一些有用的编码；
   - 虽然神经网络是为了尽可能拟合训练数据，如果有正则约束，还同时要求模型尽量简单，可以防止过拟合（一般越复杂越容易过拟合）。
   - 一般自编码器隐藏层的神经元个数少于输入神经元个数，这其实就从结构上阻止了编码和解码的恒等映射。
   - 从对隐层数据维度进行约束的角度来看，常见情况为：隐层维度小于输入数据维度、隐层维度大于输入数据维度；
   - 对于隐层维度小于输入数据维度，是一种降维操作，当每两层之间的变换均为线性，且监督训练的误差是二次型误差时，网络等价于PCA。
   - 当隐层维度大于输入维度时，稀疏自编码器约束隐层的表达尽量稀疏（即，有大量维度为0，未被激活）。这样稀疏表达是因为有研究者从人脑机理对比——人类神经系统
     在某一刺激下，大部分神经元是被抑制的，这意味着系统在尝试进行特征选择，找出大量维度中真正重要的若干维。

2. 堆叠自编码器
   - 深度学习的核心在于逐层学习原始数据的多种表达；
   - 堆叠自编码器也是在做层级学习，
   - 自编码器的解码器只是为了辅助训练出编码器，想要的是编码；
   - 堆叠自编码器就是多个自编码器的堆叠，后一个自编码器对前一个自编码器的隐层特征进一步编码，层层编码；
   - 深度学习：learning multiple levels of representation and abstraction.
   - 堆叠自编码器的训练是逐层进行的，不是一步到位的，即layer-wise unsupervised pre-training,这导致了2006年深度学习的第3次兴起。

3. 自编码器变体
   - 隐层维度如何确定？为什么稀疏的特征比较好？什么样的表达representation是好的表达？
   - 从不同角度思考，出现了不同的变体；
   - 从“什么样的特征是好的？标准”这个角度，出现了
     sparsity-sparse AE
     denoise-denoising AE
     regularization-regularized AE
     repr reg- contractive AE
     marginalize-marginalized DAE

4. 稀疏自编码器
   - 核心思想是：高维而稀疏的表达是好的；
   - 一般不会指定隐层中哪些节点是被抑制的，而是指定一个稀疏性参数p，代表隐层神经元的平均活跃程度（在训练集上取平均）；
   - 从数学模型角度上的实现方法；只要引入一个度量来衡量神经元的实际激活度与期望激活度之间的差异，将其添加到目标函数作为正则，训练网络；
     度量?概率论和信息论的概念“相对熵”，即“KL散度(KL divergence)”，当实际激活度与期望激活度之间有偏差，误差便急剧增大。

5. 降噪自编码器
   - 核心：恢复原信号未必最好，能够对“被污染/破坏”的原始数据编码，解码，还能恢复真正的原始数据，这样的特征最好；
   - DAE原理图，搜索“降噪自编码器原理图”。
   - 对加入噪声的信号进行编码和解码，损失函数是原始信号和解码结果的平方差。

6. 逐层预训练
   - 预训练好的网络在一定程度上拟合了训练数据的结构，这使得整个网络的初始值在一个合适的状态，
     便于有监督阶段加快迭代收敛。
   - 不少研究提出了很好的初始化策略，常用的dropout,relu，直接训练深层网络已经不是问题。
   - Stacks of unsupervised feature learning layers are STILL useful when you are in a regime
     with insufficient labeled examples,for transfer learning or domain adaptation.It is a
     regularizer.But when the number of labeled examples becomes large enough,the advantage of
     that regularizer becomes much less.I suspect however that this story is far from ended!There
     are other ways besides pre-training of combining supervised and unsupervised learning,and I
     believe that we still have a lot to improve in terms of our unsupervised learning algorithms.
</pre></div>
</div>
<figure class="align-center" id="id7">
<span id="id4"></span><img alt="_images/DAE.png" src="_images/DAE.png" />
<figcaption>
<p><span class="caption-text"><strong>降噪自编码器原理图</strong></span><a class="headerlink" href="#id7" title="永久链接至图片">¶</a></p>
</figcaption>
</figure>
<hr class="docutils" />
<p id="no2"><strong>NO2: CS294A Lecture notes_PDF阅读笔记</strong>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">1.</span><span class="n">Introduction</span>
<span class="mf">2.</span><span class="n">Neural</span> <span class="n">Networks</span><span class="p">(</span><span class="mf">2.1</span> <span class="n">neural</span> <span class="n">network</span> <span class="n">formulation</span><span class="p">;</span><span class="mf">2.2</span> <span class="n">backpropagation</span> <span class="n">algorithm</span><span class="p">)</span>
<span class="mf">3.</span><span class="n">AutoEncoders</span> <span class="ow">and</span> <span class="n">sparsity</span>
<span class="mf">4.</span><span class="n">Visualization</span>
<span class="mf">5.</span><span class="n">summary</span> <span class="n">of</span> <span class="n">notations</span>
</pre></div>
</div>
<ul>
<li><p><strong>Introduction</strong>:</p>
<ol class="arabic simple">
<li><p>虽然监督学习取得了巨大的成功，在很多应用上还是需要我们传递给网络好的输入特征；</p></li>
<li><p>具有良好的特征对于监督学习网络很重要；</p></li>
<li><p>有一些hand-engineered的特征虽然也很出色，但是能自动特征提取节省了大量的劳动力，属于更优秀的解决方案；</p></li>
<li><p>自编码器并不是只在计算机视觉领域出色，而是在文本、音频等领域都很优秀，这说明了编码器提取到的特征是有效的；</p></li>
<li><p>有些时候，自编码器提取的特征比hand-engineered的特征更优秀；</p></li>
<li><p>笔记接下来的组织结构。</p></li>
</ol>
</li>
<li><p><strong>Neural Networks</strong>:</p>
<ol class="arabic simple">
<li><p>神经元、网络基本互联结构，前向计算（没有有向环），随机梯度下降的反向传播算法</p></li>
<li><p>IID ：独立同分布</p></li>
<li><p>如果训练样本是从某个训练分布抽取的独立同分布点，我们可以认为算法试图最小化  损失函数的期望值【期望的变量是属于D分布的(x,y)】；</p></li>
<li><p>第3点的最小化目标函数是在样本无穷多的情况下的，在有限训练样本的情况下，最小化的目标函数是  每个样本损失的均值；</p></li>
<li><p>损失函数主要包括两项：第一项是误差平方和项；第二项是正则项（即权值衰减项weight decay term ），这一项趋向于使权重幅值下降，可以防止过拟合。
损失函数的这两项，一项针对预测与目标不符来引导权值的更新，一项通过直接对权值施加约束引导权值更新，这两项之间需要一个系数来控制两项的相对重要性；</p></li>
<li><p>say : 有假设的意思（say according to a N distribution for some small a,say 0.01）</p></li>
<li><p>随机初始化是为了打破对称性(symmetry breaking)</p></li>
<li><p>权值衰减项 一般不会用在偏置项上面，因为由对损失函数的定义可以知道，对偏置项应用这个对最终结果只有很小的影响。
权值衰减基本上是贝叶斯正则化方法（在参数上做了高斯先验，并做了MAP估计，而不是最大似然估计）的一种变体。</p></li>
<li><p>反向传播，测量每个节点对输出误差负多大的责任，误差对激活值的偏导。首先，是前向计算激活；其次，计算输出层每个节点输入与损失函数之间的偏导数；
然后，倒着往回计算每个层每个节点，误差对输入的偏导（误差分配和误差集合）；最后，根据误差对每个可调参数的偏导数，更新参数；
标量形式转化为 矩阵向量形式。注意事项包括  计算梯度所需的  值；以及梯度计算可能出现的问题（但是现在有现成的深度学习框架，梯度不需要自行编写）</p></li>
</ol>
</li>
<li><p><strong>AutoEncoders and sparsity</strong>:</p>
<ol class="arabic">
<li><p>identity function恒等函数</p></li>
<li><p>自编码器，当隐层单元数少于输入单元数，隐层被迫学到了关于输入的压缩的特征。</p></li>
<li><p>即使隐层单元数多于输入单元数，通过给隐层单元加上稀疏约束，自编码器仍能发现有趣的数据结构；</p></li>
<li><p>假设1表示神经元是活跃的，-1表示神经元是不活跃的，稀疏 即 限制 神经元大多数时候是不活跃的；</p></li>
<li><p>当训练样本是 无穷多的，我们强加的稀疏 限制可以表达为 隐层单元激活值的期望是 p，p是稀疏参数，即我们希望每个隐层神经元的预期激活是p。</p></li>
<li><p>为了满足以上的期望值，隐层单元的激活值大多数必须位于较低的一个值。</p></li>
<li><p>用于控制稀疏的算法主要包括两个方面：首先，对每个隐藏单元，要持续计算激活值的期望；其次，每次梯度迭代完，缓慢更新参数使期望值更接近设定的稀疏值。</p></li>
<li><p>“稀疏”这个术语来自于使用sigmoid激活函数网络思想的替代公式，这个激活函数的取值范围在0-1之间，在这种情况下，稀疏指的是大多数的激活值接近于0.</p></li>
<li><p>对隐层激活值的期望估计进行更新，计算公式为   新的估计值 = 旧的估计值权重 * 旧的估计值 + 新的激活值权重 * 新的激活值</p></li>
<li><p>第九条更新估计值公式 中的 权重是 算法的参数，可以分别设置为 0.999和0.001，对旧值和新值的不同权重</p></li>
<li><p>接下来就是根据  估计期望值  和 预设期望值  之间的差异，决定使 激活值变大还是变小；</p></li>
<li><dl>
<dt>根据一个神经元激活值的 计算公式，想要让激活值变小，可以减小 偏置值，反之则增大，用公式表示偏置更新，即</dt><dd><div class="math notranslate nohighlight">
\[b^{(1)}_i := b^{(1)}_i - \alpha\beta(\hat{\rho}_i - \rho)\]</div>
</dd>
</dl>
<p>也可以根据梯度公式，反向传播更新编码层的所有可训练参数，而不是只有偏置参数。
可以使用所有样本的平均值作为 实际稀疏度值，也可以像上述一样使用加权的平均值作为  实际训练过程中的稀疏度值。</p>
</li>
<li><p><strong>总结：</strong> 使用在线学习学习一个稀疏自编码器，需要3步：前向传递；反向传播；偏置更新。</p></li>
<li><dl class="simple">
<dt><strong>可视化自编码器学习到的函数：</strong></dt><dd><ul class="simple">
<li><p>每个隐层单元学习到了什么特征？==什么样的输入会导致隐层单元的最大激活；</p></li>
<li><p>对输入施加约束，在这个约束下使激活最大的输入；</p></li>
<li><p>(补充；白化是给图像去除冗余的预处理过程，通过使相邻像素变得不相关)</p></li>
<li><p>通过一个图像示例，说明了该示例中自编码器的每一个隐层单元试图学习检测图像中不同位置和方向的边缘。</p></li>
</ul>
</dd>
</dl>
</li>
</ol>
</li>
</ul>
<hr class="docutils" />
<p id="no3"><strong>NO3 : 稀疏自编码器稀疏性实现的方式</strong>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>- 稀疏是一种语言描述，在机器学习中如何公式量化表示稀疏；
- 多种惩罚项的公式可以实现网络趋向于学习稀疏特征；
</pre></div>
</div>
<p>a sparsity penalty <span class="math notranslate nohighlight">\(\Omega(h)\)</span></p>
<p>code layer <span class="math notranslate nohighlight">\(h\)</span></p>
<p>training criterion : <span class="math notranslate nohighlight">\(L(x,x')+\Omega(h)\)</span></p>
<p><span class="math notranslate nohighlight">\(h=f(Wx + b)\)</span></p>
<p><strong>method_1</strong>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>KL散度（根据隐层激活值计算得到 运行稀疏值，与期望稀疏值之间  计算KL散度）
</pre></div>
</div>
<p><span class="math notranslate nohighlight">\(\hat{\rho_j}=\frac{1}{m}\sum_{i=1}^m[h_j(x_i)]\)</span></p>
<p><span class="math notranslate nohighlight">\(\sum_{j=1}^s KL(\rho \parallel \hat{\rho_j}) = \sum_{j=1}^s [\rho log \frac{\rho}{\hat{\rho_j}} + (1-\rho) log \frac{1-\rho}{1-\hat{\rho_j}}]\)</span></p>
<p><strong>method_2</strong>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>L1和L2正则化项（对隐层激活值使用L1和L2正则）
</pre></div>
</div>
<p><span class="math notranslate nohighlight">\(L(x,x') + \lambda \sum_i |h_i|\)</span></p>
<p><strong>method_3</strong>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>手动使最大K个值的隐层单元不变，其他隐层单元的值调整为0.(k-sparse autoencoder )
</pre></div>
</div>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">  1</span>
<span class="normal">  2</span>
<span class="normal">  3</span>
<span class="normal">  4</span>
<span class="normal">  5</span>
<span class="normal">  6</span>
<span class="normal">  7</span>
<span class="normal">  8</span>
<span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="c1"># %% [markdown]</span>
<span class="c1"># #### 数据和标签（中介轴承频谱数据）</span>
<span class="c1"># 每个.npz数据文件中只有一个关键词，为&#39;data&#39;，.npz数据对.mat中数据做了转置</span>
<span class="c1"># 全为频谱数据</span>
<span class="c1"># &lt;**每个文件中的数据类型**&gt;</span>
<span class="c1"># 正常（1-200）、外圈（201-400）、内圈（401-600）、滚动体（601-800）</span>
<span class="c1"># 每个文件中都有800条数据，每条数据长度为8192</span>

<span class="c1"># %%</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>

<span class="n">data_path</span> <span class="o">=</span> <span class="s2">&quot;/home/u2020200708/DATASETS/中介轴承频谱数据（崔）/dataNOIR300_600pinpu.npz&quot;</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">data_path</span><span class="p">)[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
<span class="c1"># print(data.shape)</span>
<span class="c1"># print(np.max(data),np.min(data)) # 检查结果，输入在[0-1]之间，使用relu的激活函数可以编码解码输入数据</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">][:],</span><span class="n">data</span><span class="p">[</span><span class="mi">200</span><span class="p">:</span><span class="mi">220</span><span class="p">][:],</span><span class="n">data</span><span class="p">[</span><span class="mi">400</span><span class="p">:</span><span class="mi">420</span><span class="p">][:],</span><span class="n">data</span><span class="p">[</span><span class="mi">600</span><span class="p">:</span><span class="mi">620</span><span class="p">]))</span>
<span class="n">val_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">data</span><span class="p">[</span><span class="mi">60</span><span class="p">:</span><span class="mi">80</span><span class="p">][:],</span><span class="n">data</span><span class="p">[</span><span class="mi">260</span><span class="p">:</span><span class="mi">280</span><span class="p">][:],</span><span class="n">data</span><span class="p">[</span><span class="mi">460</span><span class="p">:</span><span class="mi">480</span><span class="p">][:],</span><span class="n">data</span><span class="p">[</span><span class="mi">660</span><span class="p">:</span><span class="mi">680</span><span class="p">]))</span> 
<span class="n">test_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">data</span><span class="p">[</span><span class="mi">100</span><span class="p">:</span><span class="mi">200</span><span class="p">][:],</span><span class="n">data</span><span class="p">[</span><span class="mi">300</span><span class="p">:</span><span class="mi">400</span><span class="p">][:],</span><span class="n">data</span><span class="p">[</span><span class="mi">500</span><span class="p">:</span><span class="mi">600</span><span class="p">][:],</span><span class="n">data</span><span class="p">[</span><span class="mi">700</span><span class="p">:</span><span class="mi">800</span><span class="p">]))</span> 

<span class="n">train_label</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">)],</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">val_label</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">)],</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">test_label</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">)],</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># CWRU</span>
<span class="c1"># data_path = &quot;/home/u2020200708/SparseAE/DenseSparseAE_L1/EXPERIMENTS/CWRU/time_domain_test1/data&quot;</span>
<span class="c1"># train_data = np.load(data_path + &quot;/trainset.npz&quot;)[&quot;train_data&quot;]</span>
<span class="c1"># val_data = np.load(data_path + &quot;/validate.npz&quot;)[&quot;validate_data&quot;]</span>
<span class="c1"># test_data = np.load(data_path + &quot;/testset.npz&quot;)[&quot;test_data&quot;]</span>

<span class="c1"># train_label = np.load(data_path + &quot;/trainset.npz&quot;)[&quot;train_label&quot;]</span>
<span class="c1"># val_label = np.load(data_path + &quot;/validate.npz&quot;)[&quot;validate_label&quot;]</span>
<span class="c1"># test_label = np.load(data_path + &quot;/testset.npz&quot;)[&quot;test_label&quot;]</span>

<span class="c1"># %% [markdown]</span>
<span class="c1"># #### 对隐层激活值使用L1正则化 进行稀疏控制</span>
<span class="c1"># 使用一维卷积的稀疏自编码器</span>
<span class="c1"># 损失函数使用mse，激活函数使用RELU，优化器使用Adam来训练自编码器</span>
<span class="c1"># 之后使用交叉熵损失，加上分类层，进行分类任务</span>
<span class="c1"># 逐层贪婪训练</span>

<span class="c1"># %%</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Conv1D</span><span class="p">,</span><span class="n">Input</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">denseSparseAE_train</span><span class="p">(</span>
    <span class="n">seq_length</span><span class="p">,</span>
    <span class="n">hidden_nums</span><span class="p">,</span>
    <span class="n">train_data</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">,</span>
    <span class="n">epochs</span><span class="p">,</span>
    <span class="n">val_data</span><span class="p">,</span>
    <span class="n">save_root</span>
<span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Construct and train *Stacked Sparse AutoEncoder*.</span>

<span class="sd">    ** Params:**</span>
<span class="sd">    -------------------</span>

<span class="sd">    - seq_length:</span>

<span class="sd">        int,length of the 1-d samples.</span>

<span class="sd">    - hidden_nums:</span>

<span class="sd">        list,number of neurons for each hidden dense layer.</span>

<span class="sd">    &#39;&#39;&#39;</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    -------------------------</span>
<span class="sd">    存储容器</span>
<span class="sd">    --------------------------</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">inputs_nums</span> <span class="o">=</span> <span class="p">[</span><span class="n">seq_length</span><span class="p">]</span> <span class="o">+</span> <span class="n">hidden_nums</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># 每个自编码器的输入维度</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">inputs_nums</span><span class="p">)</span>
    <span class="n">subAE_history</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">subAE_model</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    ----------------------------------</span>
<span class="sd">    循环构建训练单个稀疏自编码器</span>
<span class="sd">    -----------------------------------</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hidden_nums</span><span class="p">)):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">inputs_nums</span><span class="p">[</span><span class="n">i</span><span class="p">],))</span> <span class="c1"># shape is a tuple,not including the batch size.</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">hidden_nums</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span><span class="n">activity_regularizer</span><span class="o">=</span><span class="s2">&quot;l2&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># 隐层加入激活值L1正则化器</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">inputs_nums</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># relu结果只有正数，但是输入是有正有负的，所以需要把输入在训练前先归一化到0-1之间</span>

        <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;DenseSparseAE&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">))</span> <span class="c1"># 得到一个全连接的稀疏自编码器</span>

        <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;adam&quot;</span><span class="p">,</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">)</span> <span class="c1"># 这种输出拟合输入没有“准确率”一说，分类任务有</span>

        <span class="n">H</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span><span class="n">train_data</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span> <span class="n">batch_size</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span><span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">val_data</span><span class="p">,</span><span class="n">val_data</span><span class="p">))</span>

        <span class="n">subAE_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">H</span><span class="p">)</span> <span class="c1"># 记录子编码器 训练的历史记录</span>
        <span class="n">subAE_model</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="p">)</span> <span class="c1"># 记录子编码器的 模型</span>

        <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">hidden_nums</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">sub_coder</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">input</span><span class="p">,</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">)</span> <span class="c1"># 得到子编码器的“隐层输出模型”，预测输出作为 下一个自编码器的输入来进行逐层贪婪训练</span>
            <span class="n">train_data</span> <span class="o">=</span> <span class="n">sub_coder</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span> <span class="c1"># 下一个自编码器的训练数据</span>
            <span class="n">val_data</span> <span class="o">=</span> <span class="n">sub_coder</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">val_data</span><span class="p">)</span> <span class="c1"># 下一个自编码器的验证数据</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    ----------------------------------</span>
<span class="sd">    多层自编码器和解码器的组合、保存</span>
<span class="sd">    ----------------------------------</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># 把所有子编码器 的编码层 组合 —— 最终完整的编码器</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hidden_nums</span><span class="p">)):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">subAE_model</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">y</span><span class="p">)</span> <span class="c1"># 把层提取出来，使用functional API组合为模型</span>
    <span class="n">coder</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

    <span class="n">coder</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">save_root</span> <span class="o">+</span> <span class="s2">&quot;/coder.h5&quot;</span><span class="p">)</span> <span class="c1"># 保存编码器</span>

    <span class="c1"># 把所有子解码器 的解码层 组合 —— 最终完整的解码器</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">hidden_nums</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],))</span> <span class="c1"># 解码器输入长度 为 最后一个隐层神经元数</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hidden_nums</span><span class="p">)):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">subAE_model</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="o">-</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">](</span><span class="n">y</span><span class="p">)</span> 
    <span class="n">decoder</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

    <span class="n">decoder</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">save_root</span> <span class="o">+</span> <span class="s2">&quot;/decoder.h5&quot;</span><span class="p">)</span> <span class="c1"># 保存解码器</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    ------------------------------------------</span>
<span class="sd">    对每个子编码器的训练损失曲线画图</span>
<span class="sd">    ------------------------------------------    </span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hidden_nums</span><span class="p">)):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Activation L1-regularized Single AE _&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Epochs&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;MSE-Loss&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">subAE_history</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">],</span><span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;train_loss_&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">subAE_history</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="mi">4</span><span class="p">)))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">subAE_history</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">],</span><span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;val_loss_&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">subAE_history</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="mi">4</span><span class="p">)))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">save_root</span> <span class="o">+</span> <span class="s2">&quot;/training_curve_subAE_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;.png&quot;</span><span class="p">,</span><span class="n">dpi</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span><span class="n">bbox_inches</span> <span class="o">=</span> <span class="s2">&quot;tight&quot;</span><span class="p">)</span>


<span class="c1"># %% [markdown]</span>
<span class="c1"># #### 固定预训练编码器，使用微调新增分类器层</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Model</span><span class="p">,</span><span class="n">load_model</span> 
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Input</span><span class="p">,</span><span class="n">Dense</span><span class="p">,</span><span class="n">Dropout</span> 
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>

<span class="k">def</span> <span class="nf">classify_model_training</span><span class="p">(</span>
    <span class="n">train_data</span><span class="p">,</span>
    <span class="n">train_label</span><span class="p">,</span>
    <span class="n">val_data</span><span class="p">,</span>
    <span class="n">val_label</span><span class="p">,</span>
    <span class="n">test_data</span><span class="p">,</span> 
    <span class="n">test_label</span><span class="p">,</span>
    <span class="n">save_root</span><span class="p">,</span>
    <span class="n">seq_length</span><span class="p">,</span>
    <span class="n">classifier_dense_nums</span><span class="p">,</span>
    <span class="n">class_num</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">,</span>
    <span class="n">epochs</span><span class="p">,</span>
    <span class="n">class_names</span>
<span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    ----------</span>
<span class="sd">    Params:</span>
<span class="sd">    ----------</span>
<span class="sd">    - save_root:</span>

<span class="sd">        str,root path to store the results.</span>

<span class="sd">    - seq_length:</span>

<span class="sd">        int,the length of one-dim data,e.g.8192</span>

<span class="sd">    - classifier_dense_nums:</span>

<span class="sd">        list of neuron numbers in dense layers of classifier(except the last &#39;softmax&#39; layer),e.g.[20,10]</span>

<span class="sd">    - class_num:</span>

<span class="sd">        int,the number of classes,e.g.4</span>

<span class="sd">    - class_names:</span>

<span class="sd">        list of strings of each class name,e.g.[&quot;N&quot;,&quot;IR&quot;,&quot;OR&quot;,&quot;R&quot;]</span>

<span class="sd">    -------------</span>
<span class="sd">    Returns:</span>
<span class="sd">    ---------------</span>

<span class="sd">    summary of the classify model.</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    -------------------------</span>
<span class="sd">    构建微调分类器模型</span>
<span class="sd">    -------------------------</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># 导入模型</span>
    <span class="n">pretrained_coder_path</span> <span class="o">=</span> <span class="n">save_root</span> <span class="o">+</span> <span class="s2">&quot;/coder.h5&quot;</span> 
    <span class="n">coder</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="n">pretrained_coder_path</span><span class="p">)</span>

    <span class="c1"># 构建微调模型</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">coder</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="n">training</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="c1"># 冻结编码器的参数，设置为不可训练</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">classifier_dense_nums</span><span class="p">)):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">classifier_dense_nums</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">class_num</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">classify_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="n">x</span><span class="p">)</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    ----------------------------------</span>
<span class="sd">    编译、训练、保存分类模型</span>
<span class="sd">    ---------------------------------</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># 编译，训练，保存模型</span>
    <span class="n">classify_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;adam&quot;</span><span class="p">,</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;categorical_crossentropy&quot;</span><span class="p">,</span><span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;accuracy&quot;</span><span class="p">])</span>

    <span class="n">H</span> <span class="o">=</span> <span class="n">classify_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span><span class="n">train_label</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span><span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">val_data</span><span class="p">,</span><span class="n">val_label</span><span class="p">))</span>

    <span class="n">classify_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">save_root</span> <span class="o">+</span> <span class="s2">&quot;/classify_model.h5&quot;</span><span class="p">)</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    -----------------------------------</span>
<span class="sd">    画分类模型训练过程损失/准确率曲线图</span>
<span class="sd">    -----------------------------------</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;classify model training loss/accuracy curve&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Epochs&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Loss/Accuracy&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">H</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">],</span><span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;train_loss_&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">H</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="mi">4</span><span class="p">)))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">H</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;val_loss&quot;</span><span class="p">],</span><span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;val_loss_&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">H</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;val_loss&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="mi">4</span><span class="p">)))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">H</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;accuracy&quot;</span><span class="p">],</span><span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;train_accuracy_&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">H</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;accuracy&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="mi">4</span><span class="p">)))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">H</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;val_accuracy&quot;</span><span class="p">],</span><span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;val_accuracy_&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">H</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;val_accuracy&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="mi">4</span><span class="p">)))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">save_root</span> <span class="o">+</span> <span class="s2">&quot;/classify model training curve.png&quot;</span><span class="p">)</span>

    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    -------------------------------------------------</span>
<span class="sd">    使用测试集验证分类模型的效果，得到多个分类性能指标</span>
<span class="sd">    -------------------------------------------------</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">classify_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">test_label</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">),</span><span class="n">predictions</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">),</span><span class="n">target_names</span> <span class="o">=</span> <span class="n">class_names</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">classify_model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
<span class="c1"># %% [markdown]</span>
<span class="c1"># #### 代入数据，预训练自编码器，微调整个分类模型</span>

<span class="c1"># %%</span>
<span class="n">denseSparseAE_train</span><span class="p">(</span>
    <span class="mi">8192</span><span class="p">,</span>
    <span class="p">[</span><span class="mi">12000</span><span class="p">],</span>
    <span class="n">train_data</span><span class="p">,</span>
    <span class="mi">100</span><span class="p">,</span>
    <span class="mi">100</span><span class="p">,</span>
    <span class="n">val_data</span><span class="p">,</span>
    <span class="s2">&quot;/home/u2020200708/SparseAE/DenseSparseAE_L1L2/EXPERIMENTS/中介轴承频谱数据实验/dataNOIR300_600pinpu/result_2&quot;</span><span class="p">)</span>

<span class="n">classify_model_summary</span> <span class="o">=</span> <span class="n">classify_model_training</span><span class="p">(</span>
    <span class="n">train_data</span><span class="p">,</span>
    <span class="n">train_label</span><span class="p">,</span>
    <span class="n">val_data</span><span class="p">,</span>
    <span class="n">val_label</span><span class="p">,</span>
    <span class="n">test_data</span><span class="p">,</span>
    <span class="n">test_label</span><span class="p">,</span>
    <span class="s2">&quot;/home/u2020200708/SparseAE/DenseSparseAE_L1L2/EXPERIMENTS/中介轴承频谱数据实验/dataNOIR300_600pinpu/result_2&quot;</span><span class="p">,</span>
    <span class="mi">8192</span><span class="p">,</span>
    <span class="p">[</span><span class="mi">20</span><span class="p">],</span>
    <span class="mi">4</span><span class="p">,</span>
    <span class="mi">100</span><span class="p">,</span>
    <span class="mi">600</span><span class="p">,</span>
    <span class="p">[</span><span class="s1">&#39;normal&#39;</span><span class="p">,</span><span class="s1">&#39;outer&#39;</span><span class="p">,</span><span class="s1">&#39;inner&#39;</span><span class="p">,</span><span class="s1">&#39;rolling&#39;</span><span class="p">]</span>
    <span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">classify_model_summary</span><span class="p">)</span>
</pre></div>
</td></tr></table></div>
</section>
<section id="swin-transformer">
<h2>5.2 swin Transformer<a class="headerlink" href="#swin-transformer" title="永久链接至标题">¶</a></h2>
<p><a class="reference external" href="https://zhuanlan.zhihu.com/p/360225583">屠榜各大CV任务！Swin Transformer：层次化视觉Transformer</a>
<a class="reference external" href="https://www.jianshu.com/p/0635969f478b">swin transformer理解要点</a></p>
<ul class="simple">
<li><p>文本单词和视觉图片之间的差异，针对差异和问题对症下药；</p></li>
<li><p>自注意力是局部的非重叠窗口内部计算的，允许跨窗口连接（窗口融合，patch减小，特征维度变大），移位的窗口方案带来了更高的效率；</p></li>
<li><p>分层的体系结构，能在各种尺度上建模，并且相对于图像大小具有线性计算复杂度。</p></li>
<li><p>图像领域中的low-resolution feature maps</p></li>
<li><p>(additional tip) 增加分辨率的方法-upsampling or deconvolution</p></li>
<li><p>downsampling of resoulution,指的就是特征融合（如卷积中的输出神经元减少，在基于Transformer的背景下是token的减少）</p></li>
<li><p>可以表述为 ： Transformer用于feature transformation特征转换</p></li>
<li><p>Transformer模块没有改变token的数量和特征维度；只有线性映射层linear layer在改变特征维度；patch merging layer块融合层在改变token数量，即分辨率</p></li>
<li><p>modeling power 建模能力</p></li>
<li><p>又要保持non-overlapping 窗的高效计算，又要引入cross-window connections</p></li>
<li><p>alternate交换-轮替</p></li>
<li><p><strong>！！！重点</strong></p></li>
</ul>
<p><strong>1. Shifted window partitioning in successive blocks:</strong></p>
<p>The window-based self-attention module lacks connections across windows,
which limits its modeling power. To introduce cross-window connections while maintaining
the efficient computation of non-overlapping windows ,we propose <strong>a shifted window partitioning approach
which alternates between two partitioning configurations in consecrtive Swin Transformer blocks</strong>.</p>
<p>移动窗分区方法 是 一种 在两种分区配置  之间切换的方法，在连续的Swin Transformer blocks上。</p>
<p>即分区配置只有两种，是提前设计好的，是为了保证 小窗格内部的局部自注意力计算，同时进行小窗格之间的联系，以此增强模型的建模能力。</p>
<p><strong>2. swin Transformer中的shifted window变换来的方法</strong></p>
<p>在regular window 的基础上，向右向下移动一定的像素数</p>
<p><strong>原文是：
Then,the next module adopts a windowing configuration that is shifted from that of the preceding layer,
by displacing the windows by</strong> <span class="math notranslate nohighlight">\((\lfloor \frac{M}{2} \rfloor , \lfloor \{M}{2} \rfloor)\)</span> <strong>pixels from the rergularly partitioned windows.</strong></p>
<p><strong>3. 移动窗使得窗口数量变多 &amp; 一些窗口的尺寸变小——针对这个问题的解决方案（efficient batch computation for shifted configuration）</strong></p>
<p><strong>原文是：
An issue with shifted window partitioning is that it willl result in more windows,from</strong>
<span class="math notranslate nohighlight">\(\lceil \frac{h}{M} \rceil \times \lceil \frac{w}{M} \rceil\)</span> <strong>to</strong> <span class="math notranslate nohighlight">\((\lceil \frac{h}{M} \rceil + 1) \times (\lceil \frac{w}{M} \rceil + 1)\)</span>
<strong>in the shifted configuration, and some of the windows will be smaller than</strong> <span class="math notranslate nohighlight">\(M \times M\)</span></p>
<p><strong>A naive solution is to pad the smaller windows to a size of</strong> <span class="math notranslate nohighlight">\(M \times M\)</span> <strong>and mask out the padded values when computing attention.</strong>
<strong>简单的方法就是把小窗口填充，计算注意力的时候遮盖掉填充的值，但是这个方法的计算量相当大，是原来的规则窗口的好几倍，例如</strong>
<span class="math notranslate nohighlight">\(2 \times 2 \to 3 \times 3\)</span>, <strong>大了2.25倍</strong></p>
<p><strong>在实际代码里，是通过对特征图移位，并给attention设置mask来间接实现的；在保持原有的window个数下，等价计算出结果。</strong></p>
<p>源码使用的特征图移位操作使用了 <code class="code docutils literal notranslate"><span class="pre">torch.roll</span></code> 来实现；移位操作就像是所有的块朝着  top-left 方向移动（先朝top方向移动，再朝left方向移动）；
反移位操作就是反着方向进行 <code class="code docutils literal notranslate"><span class="pre">torch.roll</span></code>.</p>
<p><strong>~~~精华~~~怎么保证所提的固定窗口计算cross-window方法（4个窗） 与  原始的移位窗计算方法（9个窗）是等价的</strong></p>
<p><a class="reference external" href="https://zhuanlan.zhihu.com/p/367111046">图解Swin Transformer</a></p>
<p>因为swin-transformer的自注意力计算是在 窗口内的，所以非窗口内的自注意力计算应该忽略掉，如下图所示：</p>
<figure class="align-center">
<img alt="_images/swin-transformer-shifted-window.png" src="_images/swin-transformer-shifted-window.png" />
</figure>
<p>上图中的红色窗格是局部窗，只计算这个局部窗格里面的注意力，左图是原始的shifted window，有9个窗格，比原始的regular partitioning window多了5个窗格，
增加了计算复杂度；因此提出了使用cyclic shift得到的右图计算 等价的注意力。具体的等价原理参考下图。</p>
<figure class="align-center">
<img alt="_images/swin-transformer-efficient-batch-computation-for-shifted-configuration.jpg" src="_images/swin-transformer-efficient-batch-computation-for-shifted-configuration.jpg" />
</figure>
<p>使用0-8对9个shifted partitioning window 得到的窗格编号，由于设计本质就是窗格内部计算局部的注意力，所以等价的窗格分割也应该只保留窗格内部的注意力计算值，
而忽略窗格之间的注意力计算值，如上图所示，即给窗格之间互注意力的位置加上mask。</p>
<p><strong>the number of patches in a window == the number of tokens in a window</strong></p>
<p><strong>tip:实数空间</strong> <span class="math notranslate nohighlight">\(\Re\)</span> <strong>右上角表示的是空间每个维度的尺寸，用</strong> <span class="math notranslate nohighlight">\(\times\)</span> <strong>分隔开来</strong>,
如 <span class="math notranslate nohighlight">\(\Re^{M^2 \times d}\)</span> 表示空间是一个二维空间，第一维的长度为 <span class="math notranslate nohighlight">\(M^2\)</span> ,第二维的长度为 <span class="math notranslate nohighlight">\(d\)</span></p>
<p><strong>相对位置编码——relative position bias——使不同的位置为坐标原点，可以得到同一个位置的不同坐标值</strong>
<strong>absolute position embedding</strong></p>
<p>including a relative position bias <span class="math notranslate nohighlight">\(B \in \Re^{M^2 \times M^2}\)</span> to each head in computing similarity:</p>
<p><span class="math notranslate nohighlight">\(Attention(Q,K,V) = SoftMax(QK^T / \sqrt{d} + B)V\)</span></p>
<p>where <span class="math notranslate nohighlight">\(Q,K,V \in \Re^{M^2 \times d}\)</span> are the query,key and value matrices;d is the query/key dimension,and <span class="math notranslate nohighlight">\(M^2\)</span> is the number of patches in a window.
Since the relative position along each axis lies in the range <span class="math notranslate nohighlight">\([-M+1,M-1]\)</span>,we parameterize a smaller-sized bias matrix <span class="math notranslate nohighlight">\(\hat{B} \in \Re^{(2M-1) \times (2M-1))}\)</span>
and values in B are taken from <span class="math notranslate nohighlight">\(\hat{B}\)</span></p>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">XIA's BLOG</a></h1>








<h3>导航</h3>
<p><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="How_to_build_a_blog_C1.html">第一章 知乎学习搭建博客</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E4%BF%A1%E5%8F%B7%E7%9B%B8%E5%85%B3_C2.html">第二章 信号分解</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3_C3.html">第三章 python篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E7%94%B5%E8%84%91%E6%93%8D%E4%BD%9C%E7%9B%B8%E5%85%B3_C4.html">第四章 电脑操作</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">第五章 机器学习</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id2">5.1 稀疏自编码器</a></li>
<li class="toctree-l2"><a class="reference internal" href="#swin-transformer">5.2 swin Transformer</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB_C6.html">第六章 文献阅读笔记</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E6%95%B0%E6%8D%AE%E9%9B%86_C7.html">第七章 数据集介绍说明</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E6%9C%9F%E5%88%8A%E4%BF%A1%E6%81%AF_C8.html">第八章 期刊信息</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E6%9C%BA%E6%A2%B0%E7%9B%B8%E5%85%B3_C9.html">第九章 机械</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E7%90%90%E7%A2%8E_C10.html">第十章 琐碎</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="%E7%94%B5%E8%84%91%E6%93%8D%E4%BD%9C%E7%9B%B8%E5%85%B3_C4.html" title="上一章">第四章 电脑操作</a></li>
      <li>Next: <a href="%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB_C6.html" title="下一章">第六章 文献阅读笔记</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">快速搜索</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="转向" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2021, xia liu.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.5.3</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/机器学习_C5.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>