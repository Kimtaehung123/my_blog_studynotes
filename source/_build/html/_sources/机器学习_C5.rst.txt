第五章 机器学习
================

5.1 稀疏自编码器
------------------

**References:**

`[1]CS294A Lecture notes <http://web.stanford.edu/class/archive/cs/cs294a/cs294a.1104/sparseAutoencoder.pdf>`_  

`[2]一文看懂自编码器、堆叠自编码器、稀疏自编码器、降噪自编码器 <https://www.zhihu.com/question/41490383/answer/103006793>`_

`[3]Regularized_autoencoders —— Sparse Autoencoder <https://en.m.wikipedia.org/wiki/Autoencoder#Regularized_autoencoders>`_

-----------------------------------------------------------------

**ContentJumpTo:**

- 降噪自编码器原理图_ 
- NO1_
- NO2_
- NO3_


-------------------------------------------------------

**Contents:** 

.. _NO1:

**NO1: 一文看懂自编码器、堆叠自编码器、稀疏自编码器、降噪自编码器**::

    1. 自编码器
       - 如果编码和解码的函数是恒等映射，那么自编码器相当于没有学习到任何新的东西，自始至终只有输入信息；
       - 基于上一点，为了防止恒等映射的发生，我们经常对中间信号（编码）做一定的约束，这样系统能学到一些有用的编码；
       - 虽然神经网络是为了尽可能拟合训练数据，如果有正则约束，还同时要求模型尽量简单，可以防止过拟合（一般越复杂越容易过拟合）。
       - 一般自编码器隐藏层的神经元个数少于输入神经元个数，这其实就从结构上阻止了编码和解码的恒等映射。
       - 从对隐层数据维度进行约束的角度来看，常见情况为：隐层维度小于输入数据维度、隐层维度大于输入数据维度；
       - 对于隐层维度小于输入数据维度，是一种降维操作，当每两层之间的变换均为线性，且监督训练的误差是二次型误差时，网络等价于PCA。
       - 当隐层维度大于输入维度时，稀疏自编码器约束隐层的表达尽量稀疏（即，有大量维度为0，未被激活）。这样稀疏表达是因为有研究者从人脑机理对比——人类神经系统
         在某一刺激下，大部分神经元是被抑制的，这意味着系统在尝试进行特征选择，找出大量维度中真正重要的若干维。

    2. 堆叠自编码器
       - 深度学习的核心在于逐层学习原始数据的多种表达；
       - 堆叠自编码器也是在做层级学习，
       - 自编码器的解码器只是为了辅助训练出编码器，想要的是编码；
       - 堆叠自编码器就是多个自编码器的堆叠，后一个自编码器对前一个自编码器的隐层特征进一步编码，层层编码；
       - 深度学习：learning multiple levels of representation and abstraction.
       - 堆叠自编码器的训练是逐层进行的，不是一步到位的，即layer-wise unsupervised pre-training,这导致了2006年深度学习的第3次兴起。

    3. 自编码器变体
       - 隐层维度如何确定？为什么稀疏的特征比较好？什么样的表达representation是好的表达？
       - 从不同角度思考，出现了不同的变体；
       - 从“什么样的特征是好的？标准”这个角度，出现了
         sparsity-sparse AE 
         denoise-denoising AE 
         regularization-regularized AE 
         repr reg- contractive AE 
         marginalize-marginalized DAE 

    4. 稀疏自编码器
       - 核心思想是：高维而稀疏的表达是好的；
       - 一般不会指定隐层中哪些节点是被抑制的，而是指定一个稀疏性参数p，代表隐层神经元的平均活跃程度（在训练集上取平均）；
       - 从数学模型角度上的实现方法；只要引入一个度量来衡量神经元的实际激活度与期望激活度之间的差异，将其添加到目标函数作为正则，训练网络；
         度量?概率论和信息论的概念“相对熵”，即“KL散度(KL divergence)”，当实际激活度与期望激活度之间有偏差，误差便急剧增大。

    5. 降噪自编码器
       - 核心：恢复原信号未必最好，能够对“被污染/破坏”的原始数据编码，解码，还能恢复真正的原始数据，这样的特征最好；
       - DAE原理图，搜索“降噪自编码器原理图”。
       - 对加入噪声的信号进行编码和解码，损失函数是原始信号和解码结果的平方差。

    6. 逐层预训练
       - 预训练好的网络在一定程度上拟合了训练数据的结构，这使得整个网络的初始值在一个合适的状态，
         便于有监督阶段加快迭代收敛。
       - 不少研究提出了很好的初始化策略，常用的dropout,relu，直接训练深层网络已经不是问题。
       - Stacks of unsupervised feature learning layers are STILL useful when you are in a regime
         with insufficient labeled examples,for transfer learning or domain adaptation.It is a 
         regularizer.But when the number of labeled examples becomes large enough,the advantage of 
         that regularizer becomes much less.I suspect however that this story is far from ended!There
         are other ways besides pre-training of combining supervised and unsupervised learning,and I 
         believe that we still have a lot to improve in terms of our unsupervised learning algorithms.

         

.. _降噪自编码器原理图:

.. figure::
   images\\机器学习\\DAE.png
   :align: center

   **降噪自编码器原理图**

-------------------------------------------------------------

.. _NO2:

**NO2: CS294A Lecture notes_PDF阅读笔记**::

    1.Introduction
    2.Neural Networks(2.1 neural network formulation;2.2 backpropagation algorithm)
    3.AutoEncoders and sparsity
    4.Visualization
    5.summary of notations

- **Introduction**:
      
  1. 虽然监督学习取得了巨大的成功，在很多应用上还是需要我们传递给网络好的输入特征；
  2. 具有良好的特征对于监督学习网络很重要；
  3. 有一些hand-engineered的特征虽然也很出色，但是能自动特征提取节省了大量的劳动力，属于更优秀的解决方案；
  4. 自编码器并不是只在计算机视觉领域出色，而是在文本、音频等领域都很优秀，这说明了编码器提取到的特征是有效的；
  5. 有些时候，自编码器提取的特征比hand-engineered的特征更优秀；
  6. 笔记接下来的组织结构。

- **Neural Networks**:
  
  1. 神经元、网络基本互联结构，前向计算（没有有向环），随机梯度下降的反向传播算法
  2. IID ：独立同分布
  3. 如果训练样本是从某个训练分布抽取的独立同分布点，我们可以认为算法试图最小化  损失函数的期望值【期望的变量是属于D分布的(x,y)】；
  4. 第3点的最小化目标函数是在样本无穷多的情况下的，在有限训练样本的情况下，最小化的目标函数是  每个样本损失的均值；
  5. 损失函数主要包括两项：第一项是误差平方和项；第二项是正则项（即权值衰减项weight decay term ），这一项趋向于使权重幅值下降，可以防止过拟合。
     损失函数的这两项，一项针对预测与目标不符来引导权值的更新，一项通过直接对权值施加约束引导权值更新，这两项之间需要一个系数来控制两项的相对重要性；
  6. say : 有假设的意思（say according to a N distribution for some small a,say 0.01）
  7. 随机初始化是为了打破对称性(symmetry breaking)
  8. 权值衰减项 一般不会用在偏置项上面，因为由对损失函数的定义可以知道，对偏置项应用这个对最终结果只有很小的影响。
     权值衰减基本上是贝叶斯正则化方法（在参数上做了高斯先验，并做了MAP估计，而不是最大似然估计）的一种变体。
  9. 反向传播，测量每个节点对输出误差负多大的责任，误差对激活值的偏导。首先，是前向计算激活；其次，计算输出层每个节点输入与损失函数之间的偏导数；
     然后，倒着往回计算每个层每个节点，误差对输入的偏导（误差分配和误差集合）；最后，根据误差对每个可调参数的偏导数，更新参数；
     标量形式转化为 矩阵向量形式。注意事项包括  计算梯度所需的  值；以及梯度计算可能出现的问题（但是现在有现成的深度学习框架，梯度不需要自行编写）

- **AutoEncoders and sparsity**:
  
  1. identity function恒等函数
  2. 自编码器，当隐层单元数少于输入单元数，隐层被迫学到了关于输入的压缩的特征。
  3. 即使隐层单元数多于输入单元数，通过给隐层单元加上稀疏约束，自编码器仍能发现有趣的数据结构；
  4. 假设1表示神经元是活跃的，-1表示神经元是不活跃的，稀疏 即 限制 神经元大多数时候是不活跃的；
  5. 当训练样本是 无穷多的，我们强加的稀疏 限制可以表达为 隐层单元激活值的期望是 p，p是稀疏参数，即我们希望每个隐层神经元的预期激活是p。
  6. 为了满足以上的期望值，隐层单元的激活值大多数必须位于较低的一个值。
  7. 用于控制稀疏的算法主要包括两个方面：首先，对每个隐藏单元，要持续计算激活值的期望；其次，每次梯度迭代完，缓慢更新参数使期望值更接近设定的稀疏值。
  8. “稀疏”这个术语来自于使用sigmoid激活函数网络思想的替代公式，这个激活函数的取值范围在0-1之间，在这种情况下，稀疏指的是大多数的激活值接近于0.
  9. 对隐层激活值的期望估计进行更新，计算公式为   新的估计值 = 旧的估计值权重 * 旧的估计值 + 新的激活值权重 * 新的激活值
  10. 第九条更新估计值公式 中的 权重是 算法的参数，可以分别设置为 0.999和0.001，对旧值和新值的不同权重
  11. 接下来就是根据  估计期望值  和 预设期望值  之间的差异，决定使 激活值变大还是变小；
  12. 根据一个神经元激活值的 计算公式，想要让激活值变小，可以减小 偏置值，反之则增大，用公式表示偏置更新，即
                       .. math:: b^{(1)}_i := b^{(1)}_i - \alpha\beta(\hat{\rho}_i - \rho)
      也可以根据梯度公式，反向传播更新编码层的所有可训练参数，而不是只有偏置参数。
      可以使用所有样本的平均值作为 实际稀疏度值，也可以像上述一样使用加权的平均值作为  实际训练过程中的稀疏度值。
  13. **总结：** 使用在线学习学习一个稀疏自编码器，需要3步：前向传递；反向传播；偏置更新。
  14. **可视化自编码器学习到的函数：**
          - 每个隐层单元学习到了什么特征？==什么样的输入会导致隐层单元的最大激活；
          - 对输入施加约束，在这个约束下使激活最大的输入；
          - (补充；白化是给图像去除冗余的预处理过程，通过使相邻像素变得不相关)
          - 通过一个图像示例，说明了该示例中自编码器的每一个隐层单元试图学习检测图像中不同位置和方向的边缘。

---------------------------------------------------------

.. _NO3:

**NO3 : 稀疏自编码器稀疏性实现的方式**::

   - 稀疏是一种语言描述，在机器学习中如何公式量化表示稀疏；
   - 多种惩罚项的公式可以实现网络趋向于学习稀疏特征；
  
a sparsity penalty :math:`\Omega(h)`

code layer :math:`h`

training criterion : :math:`L(x,x')+\Omega(h)`

:math:`h=f(Wx + b)`

**method_1**::

   KL散度（根据隐层激活值计算得到 运行稀疏值，与期望稀疏值之间  计算KL散度）

:math:`\hat{\rho_j}=\frac{1}{m}\sum_{i=1}^m[h_j(x_i)]`

:math:`\sum_{j=1}^s KL(\rho \parallel \hat{\rho_j}) = \sum_{j=1}^s [\rho log \frac{\rho}{\hat{\rho_j}} + (1-\rho) log \frac{1-\rho}{1-\hat{\rho_j}}]`
  
**method_2**::

   L1和L2正则化项（对隐层激活值使用L1和L2正则）

:math:`L(x,x') + \lambda \sum_i |h_i|`

**method_3**::
   
   手动使最大K个值的隐层单元不变，其他隐层单元的值调整为0.(k-sparse autoencoder )


.. literalinclude:: SparseAutoEncoder.py
   :linenos:
   :language: python
   :lines: 1-

