5.4.1 vanilla transformer 
--------------------------

5.4.1.1 参考
--------------

`vanilla transformer <https://blog.csdn.net/sinat_37574187/article/details/119890682>`_

`论文笔记-Vanilla Transformer：Character-Level Language Modeling with Deeper Self-Attention <https://blog.csdn.net/u010366748/article/details/114301942>`_

5.4.1.2 详情
---------------

* Vanilla Transformer论文中使用64层模型，并仅限于处理 512个字符这种相对较短的输入，因此它将输入分成段，并分别从每个段中进行学习，如上图所示。 在测试阶段如需处理较长的输入，该模型会在每一步中将输入向右移动一个字符，以此实现对单个字符的预测。

* Vanilla Transformer的三个缺点：

    * 上下文长度受限：字符之间的最大依赖距离受输入长度的限制，模型看不到出现在几个句子之前的单词。  
    * 上下文碎片：对于长度超过512个字符的文本，都是从头开始单独训练的。段与段之间没有上下文依赖性，会让训练效率低下，也会影响模型的性能。
    * 推理速度慢：在测试阶段，每次预测下一个单词，都需要重新构建一遍上下文，并从头开始计算，这样的计算速度非常慢。
