<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>5.3.1 损失函数相关 &mdash; XIA&#39;s BLOG 1.0 文档</title><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/translations.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="索引" href="genindex.html" />
    <link rel="search" title="搜索" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> XIA's BLOG
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p><span class="caption-text">目录：</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="How_to_build_a_blog_C1.html">第一章 知乎学习搭建博客</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E4%BF%A1%E5%8F%B7%E7%9B%B8%E5%85%B3_C2.html">第二章 信号分解</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3_C3.html">第三章 python篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E7%94%B5%E8%84%91%E6%93%8D%E4%BD%9C%E7%9B%B8%E5%85%B3_C4.html">第四章 电脑操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_C5.html">第五章 机器学习</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB_C6.html">第六章 文献阅读笔记</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E6%95%B0%E6%8D%AE%E9%9B%86_C7.html">第七章 数据集介绍说明</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E6%9C%9F%E5%88%8A%E4%BF%A1%E6%81%AF_C8.html">第八章 期刊信息</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E6%9C%BA%E6%A2%B0%E7%9B%B8%E5%85%B3_C9.html">第九章 机械</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E7%88%AC%E8%99%AB%E7%9B%B8%E5%85%B3_C10.html">python 爬虫</a></li>
<li class="toctree-l1"><a class="reference internal" href="2023%E6%A0%A1%E6%8B%9B%E6%B1%82%E8%81%8C-%E7%AC%94%E8%AF%95%26%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95_C11.html">第十一章 2023校招求职-笔试&amp;面试记录</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">XIA's BLOG</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>5.3.1 损失函数相关</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/损失函数531.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="id1">
<h1>5.3.1 损失函数相关<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h1>
<p>损失函数的输入是真实标签和预测值，一般需要 <strong>预设（初始化）</strong>
* 预测值是否from_logits（如果是，则应用反函数sigmoid函数得到概率值，再进行损失的计算）；
* 沿着哪个轴计算损失，一般是最后一个维度（计算损失就是减少一个维度）；
* 应用到损失值的reduction方法（一般有auto根据上下文语境选择；大部分情况是sum_over_batch_size；当使用分布式策略时，这两种reduction方法报错）；
* 标签平滑label smoothing <a class="reference external" href="https://blog.csdn.net/qq_43211132/article/details/100510113">参考</a>;
* 算符Op的命名</p>
<p><strong>调用</strong> （以tensorflow为例）
* 真实标签
* 预测值
* 权值(可以是一个标量用于损失值整体的缩放；或者是一个向量用于对不同的batch进行加权；或者是更高维的向量（广播机制），用于更细致的加权)</p>
<p><strong>类方法</strong>
* from_config(config_dict): 输入配置参数，返回损失实例化对象
* get_config(): 返回损失实例的配置字典
* __call__(y_true,y_pred,sample_weights): 激活损失实例对象，返回加权浮点损失tensor(reduction不是None的时候，否则为向量)</p>
<p><strong>appendix : label smoothing</strong>
* 一种正则化方法
* 防止过拟合（数据或多或少存在一定的噪声；训练数据只是使用了部分数据，模型的学习能力太强容易过拟合）
* 公式为 <span class="math notranslate nohighlight">\(outputs = (1-\epsilon) * inputs + \frac{\epsilon}{k}\)</span> （inputs是真实标签，outputs是平滑后的标签）
* <a class="reference external" href="https://blog.csdn.net/weixin_44305115/article/details/106605237?spm=1001.2101.3001.6661.1&amp;utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1.pc_relevant_paycolumn_v3&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1.pc_relevant_paycolumn_v3&amp;utm_relevant_index=1">一个小例子搞懂transformer中的label smoothing(标签平滑)</a></p>
<ol class="arabic simple">
<li><p>CategoricalCrossentropy</p>
<ul class="simple">
<li><dl class="simple">
<dt><strong>类别：</strong></dt><dd><p>损失函数losses</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>基本说明：</strong></dt><dd><ul>
<li><p>用于分类问题</p></li>
<li><p>二类及以上类别数问题的分类；</p></li>
<li><p>多分类交叉熵</p></li>
<li><p>标签是one-hot表示</p></li>
<li><p>真实标签与预测值的形状都为（批量数，分类数）</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>公式：</strong></dt><dd><ul>
<li><p><span class="math notranslate nohighlight">\(loss = - \frac{1}{n} \sum_{i = 1}^{n} t_k (ln p_k)\)</span></p></li>
<li><p>n是样本数，</p></li>
<li><p><span class="math notranslate nohighlight">\(t_k\)</span> 是真实类别k的二值标签（为1），</p></li>
<li><p><span class="math notranslate nohighlight">\(p_k\)</span> 是对真实类别k的预测概率</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>参考资料：</strong></dt><dd><p><a class="reference external" href="https://blog.csdn.net/At_a_lost/article/details/109531309">机器学习常见损失函数，二元交叉熵，类别交叉熵，MSE，稀疏类别交叉熵</a></p>
</dd>
</dl>
</li>
</ul>
</li>
<li><p>MSE-均方误差</p>
<ul class="simple">
<li><dl class="simple">
<dt><strong>类别：</strong></dt><dd><p>损失函数losses</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>基本说明：</strong></dt><dd><ul>
<li><p>主要用于回归问题</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>公式：</strong></dt><dd><ul>
<li><p><span class="math notranslate nohighlight">\(loss = \frac{1}{n} \sum_{i = 1}^{n} (p_i - t_i)^2\)</span></p></li>
<li><p>n是样本数，</p></li>
<li><p><span class="math notranslate nohighlight">\(t_i\)</span> 是标签，</p></li>
<li><p><span class="math notranslate nohighlight">\(p_i\)</span> 是预测值；</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>参考资料：</strong></dt><dd><p><a class="reference external" href="https://blog.csdn.net/At_a_lost/article/details/109531309">机器学习常见损失函数，二元交叉熵，类别交叉熵，MSE，稀疏类别交叉熵</a></p>
</dd>
</dl>
</li>
</ul>
</li>
<li><p>RMSE-均方根误差</p>
<ul class="simple">
<li><dl class="simple">
<dt><strong>类别：</strong></dt><dd><p>损失函数losses</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>基本说明：</strong></dt><dd><ul>
<li><p>主要用于回归问题</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>公式：</strong></dt><dd><ul>
<li><p><span class="math notranslate nohighlight">\(loss = \sqrt{\frac{1}{n} \sum_{i = 1}^{n} (p_i - t_i)^2}\)</span></p></li>
<li><p>n是样本数，</p></li>
<li><p><span class="math notranslate nohighlight">\(t_i\)</span> 是标签，</p></li>
<li><p><span class="math notranslate nohighlight">\(p_i\)</span> 是预测值；</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>参考资料：</strong></dt><dd><p><a class="reference external" href="https://blog.csdn.net/At_a_lost/article/details/109531309">机器学习常见损失函数，二元交叉熵，类别交叉熵，MSE，稀疏类别交叉熵</a></p>
</dd>
</dl>
</li>
</ul>
</li>
<li><p>BinaryCrossentropy</p>
<ul class="simple">
<li><dl class="simple">
<dt><strong>类别：</strong></dt><dd><p>损失函数losses</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>基本说明：</strong></dt><dd><ul>
<li><p>用于分类问题</p></li>
<li><p>二分类交叉熵</p></li>
<li><p>from_logits，指的是预测值与概率值符合logit函数关系，
预测值是来自logit函数（即预测值是属于-INF~+INF范围内），所以通过对预测值做logit反函数变换（即加一个sigmoid函数），可以得到概率值(0,1)</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>公式：</strong></dt><dd><ul>
<li><p><span class="math notranslate nohighlight">\(loss = - \frac{1}{n} \sum_{i = 1}^{n} (t_i (ln p_i) + (1-t_i)(ln(1-p_i)))\)</span></p></li>
<li><p>n是计算一次损失时所有样本的所有输出数，</p></li>
<li><p><span class="math notranslate nohighlight">\(t_i\)</span> 是真实二值标签，</p></li>
<li><p><span class="math notranslate nohighlight">\(p_i\)</span> 是预测概率；</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>参考资料：</strong></dt><dd><p><a class="reference external" href="https://blog.csdn.net/At_a_lost/article/details/109531309">机器学习常见损失函数，二元交叉熵，类别交叉熵，MSE，稀疏类别交叉熵</a>
<a class="reference external" href="https://zhuanlan.zhihu.com/p/181553413">Binary Cross Entropy Loss</a></p>
</dd>
</dl>
</li>
</ul>
</li>
<li><p>SparseCategoricalCrossentropy</p>
<ul class="simple">
<li><dl class="simple">
<dt><strong>类别：</strong></dt><dd><p>损失函数losses</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>基本说明：</strong></dt><dd><ul>
<li><p>用于分类问题</p></li>
<li><p>二类及以上类别数问题的分类；</p></li>
<li><p>标签是整数表示形式；</p></li>
<li><p>如果预测形状为（批量数，类别数），那么真实标签形状应该为（批量数），真实标签空间比预测空间少一个维度</p></li>
<li><p>预测输出最后一个维度包括了各个类别的概率，标签只包含了真实的类别索引值（即，标签是整数，非one-hot形式的表示)</p></li>
<li><p>稀疏多分类交叉熵</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>公式：</strong></dt><dd><ul>
<li><p><span class="math notranslate nohighlight">\(loss = - \frac{1}{n} \sum_{i = 1}^{n} (t_k (ln p_k) + (1-t_k)(ln(1-p_k)))\)</span></p></li>
<li><p>n是样本数，</p></li>
<li><p><span class="math notranslate nohighlight">\(t_k\)</span> 是真实类别k的二值标签（为1），</p></li>
<li><p><span class="math notranslate nohighlight">\(p_k\)</span> 是对真实类别k的预测概率；</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>参考资料：</strong></dt><dd><p><a class="reference external" href="https://blog.csdn.net/At_a_lost/article/details/109531309">机器学习常见损失函数，二元交叉熵，类别交叉熵，MSE，稀疏类别交叉熵</a></p>
</dd>
</dl>
</li>
</ul>
</li>
<li><p>logit（log-odds) (abbreviation for logistic units</p>
<ul class="simple">
<li><dl class="simple">
<dt><strong>类别：</strong></dt><dd><p>概率</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>基本说明：</strong></dt><dd><ul>
<li><p>计算步骤是 ：先计算 <span class="math notranslate nohighlight">\(odds = p_出现/p_不出现\)</span> ；然后对odds取对数；即log it，其中的it指代odds</p></li>
<li><p>odds值的范围在 <span class="math notranslate nohighlight">\([0,\infty)\)</span> ；</p></li>
<li><p>logit取值范围在 <span class="math notranslate nohighlight">\((- \infty,+ \infty)\)</span></p></li>
<li><p>logit相比于出现的概率，更加强化了出现概率要高。</p></li>
<li><p>它的函数表达式的反函数是 <span class="math notranslate nohighlight">\(y = \frac{1}{1-e^{-x}}\)</span></p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>公式：</strong></dt><dd><ul>
<li><p><span class="math notranslate nohighlight">\(logit(p) = ln \frac{p}{1-p} = ln odds\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(odds = \frac{p}{1-p}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(logit^{-1}(p) = \frac{1}{1+e^(-p)}\)</span>  <strong>logit函数的反函数是sigmoid函数，把logit值输入到sigmoid函数得到概率值</strong></p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>参考资料：</strong></dt><dd><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/27188729">Logit究竟是个啥？——离散选择模型之三</a>
<a class="reference external" href="https://en.wikipedia.org/wiki/Logit">Wikipedia Logit</a></p>
</dd>
</dl>
</li>
</ul>
</li>
</ol>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 2021, xia liu.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>