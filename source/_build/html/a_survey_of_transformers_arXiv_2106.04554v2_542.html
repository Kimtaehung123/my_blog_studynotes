
<!DOCTYPE html>

<html lang="zh_CN">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17: http://docutils.sourceforge.net/" />

    <title>5.4.2 a_survey_of_transformers &#8212; XIA&#39;s BLOG 1.0 文档</title>
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/translations.js"></script>
    <link rel="index" title="索引" href="genindex.html" />
    <link rel="search" title="搜索" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="a-survey-of-transformers">
<h1>5.4.2 a_survey_of_transformers<a class="headerlink" href="#a-survey-of-transformers" title="永久链接至标题">¶</a></h1>
<ol class="arabic">
<li><p><strong>主要内容：</strong></p>
<ol class="loweralpha simple">
<li><p>vanilla Transformer &amp;  X-formers</p></li>
<li><dl class="simple">
<dt>X-formers from three perspectives:</dt><dd><ul class="simple">
<li><p>architectural modification</p></li>
<li><p>pre-training</p></li>
<li><p>applications</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>potential directions</p></li>
</ol>
</li>
<li><p><strong>关键词：</strong></p>
<ol class="loweralpha simple">
<li><p>Transformer</p></li>
<li><p>Self-Attention</p></li>
<li><p>Pre-trained Models</p></li>
<li><p>Deep Learning</p></li>
</ol>
</li>
<li><p><strong>Introduction:</strong></p>
<ul>
<li><p><em>fields</em> : NLP, CV, speech processing,Transformer-based pre-trained models (PTMs)</p></li>
<li><p><em>origin</em> : 机器翻译的序列到序列模型</p></li>
<li><p><em>X-formers improvements</em> ：These X-formers improve the vanilla Transformer from different perspectives:</p>
<blockquote>
<div><ul>
<li><p>model efficience :</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">long</span> <span class="n">sequences</span><span class="p">,</span> <span class="n">computation</span> <span class="ow">and</span> <span class="n">memory</span> <span class="n">complexity</span> <span class="n">of</span> <span class="bp">self</span><span class="o">-</span><span class="n">attention</span>
</pre></div>
</div>
</li>
<li><p>model generalization:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">makes</span> <span class="n">few</span> <span class="n">assumptions</span> <span class="n">on</span> <span class="n">the</span> <span class="n">structural</span> <span class="n">bias</span> <span class="n">of</span> <span class="nb">input</span> <span class="n">data</span><span class="p">,</span>
<span class="n">it</span> <span class="ow">is</span> <span class="n">hard</span> <span class="n">to</span> <span class="n">train</span> <span class="n">on</span> <span class="n">small</span><span class="o">-</span><span class="n">scale</span> <span class="n">data</span><span class="o">.</span>
</pre></div>
</div>
</li>
<li><p>model adaptation:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">adapt</span> <span class="n">the</span> <span class="n">Transformer</span> <span class="n">to</span> <span class="n">specific</span> <span class="n">downstream</span> <span class="n">tasks</span> <span class="ow">and</span> <span class="n">applications</span><span class="o">.</span>
</pre></div>
</div>
</li>
</ul>
</div></blockquote>
</li>
</ul>
<blockquote>
<div><ul>
<li><p><em>organization of the article</em> :</p>
<blockquote>
<div><ul class="simple">
<li><p>Sec. 2 : introduces the architecture and the key components of Transformer.</p></li>
<li><p>Sec. 3 : clarifies the categorization of Transformer variants.</p></li>
<li><p>Sec. 4∼5 : review the module-level modifications, including attention module, position encoding, layer normalization and feed-forward layer.</p></li>
<li><p>Sec. 6 : reviews the architecture-level variants.</p></li>
<li><p>Sec. 7 : introduces some of the representative Transformer-based PTMs.</p></li>
<li><p>Sec. 8 : introduces the application of Transformer to various different fields.</p></li>
<li><p>Sec. 9 : discusses some aspects of Transformer that researchers might find intriguing and summarizes the paper.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</li>
</ol>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">XIA's BLOG</a></h1>








<h3>导航</h3>
<p><span class="caption-text">目录：</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="How_to_build_a_blog_C1.html">第一章 知乎学习搭建博客</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E4%BF%A1%E5%8F%B7%E7%9B%B8%E5%85%B3_C2.html">第二章 信号分解</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3_C3.html">第三章 python篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E7%94%B5%E8%84%91%E6%93%8D%E4%BD%9C%E7%9B%B8%E5%85%B3_C4.html">第四章 电脑操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_C5.html">第五章 机器学习</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB_C6.html">第六章 文献阅读笔记</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E6%95%B0%E6%8D%AE%E9%9B%86_C7.html">第七章 数据集介绍说明</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E6%9C%9F%E5%88%8A%E4%BF%A1%E6%81%AF_C8.html">第八章 期刊信息</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E6%9C%BA%E6%A2%B0%E7%9B%B8%E5%85%B3_C9.html">第九章 机械</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E7%90%90%E7%A2%8E_C10.html">第十章 琐碎</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">快速搜索</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="转向" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2021, xia liu.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.5.3</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/a_survey_of_transformers_arXiv_2106.04554v2_542.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>